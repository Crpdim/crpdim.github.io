---
title: 机器学习课程02-浅谈机器学习原理
date: 2023/03/30
categories:
- [人工智能, 机器学习]
tags: 机器学习
cover: assets/pexels-12.jpg
math: true
---

# 宝可梦与数码宝贝的分类器-浅谈机器学习原理

> 目的：找到一个函数可以分辨宝可梦与数码宝贝

## 定一个含有未知数h的函数

### 根据画风差异分辨宝可梦与数码宝贝

1. 分析线条的复杂程度（数码宝贝较复杂， 宝可梦较简单）
   * 提取图片中的边缘线
   * 得到边缘线像素点的数量
   * 边缘像素点的数量就代表边缘的复杂程度

2. 根据此生成一个函数，函数中有一未知参数h。

   * 输入精灵图片，分析线条复杂程度
   * 若复杂程度超过一个特定的值h，归为数码宝贝

   * 若复杂程度小于特定值h， 归为宝可梦
   * 将h所有的可能性集合起来，得到H={1，2，……，10000}  !!h可能是1，2……!!
   * H内可能的h的数量即模型的++复杂程度++

## 定一个损失 Loss

1. 给出一个==数据集==，数据集中有++成对++的输入$x_n$与输出$y_n$
   * $x_n$即数据集中精灵的图片
   * $y_n$即数据集中$x_n$对应的精灵的类别label

2. 根据数据集求损失值 L

   * 根据每一对数据算得一个单个损失值$l_n$， $l_n$越大，表现越差，反之表现越好

     +++ 求一对数据损失值l的方法

     在此有很多解决办法，最简单直观的：

     * 若将$x_n$和给定h值输入到函数中，若得到相应的$y_n$，$l_n$为0
     * 若得到的结果不匹配，$l_n$为1
     * 比如：!!输入一个宝可梦的照片$x_n$， 输出的结果$y_n$为“数码宝贝”，则$l_n$为1，若输出结果为“宝可梦”则$l_n$为0!

     +++

     

   * 将所有组得到的l求和取平均获取最终损失值L

$$ \begin {array}{c}

L = \frac1 N \sum_{n=1}^N l(h, x^n, \hat y^n )

\end{array}$$

## 训练样例选择

> 找到一个具有代表性的数据集$D_{train}$

理想情况下，如果收集所有的宝可梦和数码宝贝的图片作数据集$D_{all}$，就能得到最好的 $h^{all}$ :

* $h^{all} =  argmin_h L(h, D_{all})$ !!即$h^{all}$  = 可以让L取到最小的 h!! 

现实中，我们不可能收集到所有的数码宝贝与宝可梦，只能收集到一部分图片作为数据集$D_{train}$ ，从$D_{train}$得到 $h^{train}$ :

* $h^{train} = argmin_h L(h, D_{train})$

> 我们希望$L(h^{train}, D_{all})$与 $L(h^{all},D_{all})$ 是接近的,也就是现实的损失值接近于理想的损失值，即 $L(h^{train}, D_{all}) - L(h^{all},D_{all})≤ \delta$ ，那么什么样的数据集$D_{train}$可以满足这种情况呢？

$\forall h \in H, | L(h, D_{train})  - L(h, D_{all}) | ≤  \delta / 2$ ,通常将$\delta / 2$ 。对所有可能的h而言，从数据集中算得的损失与所有的数据集中算的的损失差距小于等与$\delta / 2$，则可以得到我们想要的$L(h^{train}, D_{all})$与 $L(h^{all},D_{all})$ 相接近。



## 抽样到不好的样本的可能性

怎样定义一个坏的一个训练集$D_{train}$: 它肯定满足 ”存在至少一个h，使得 $L(h, D_{train})  - L(h, D_{all}) | >  \delta / 2$“

坏样本的概率 = 在H的范围内，由于$h_n$导致样本坏的概率 的合集，也就约等于 $h_n$导致样本坏的概率求和，即：

$$ \begin {array}{c}

P(D_{train} \ is \ bad) = \bigcup_{\substack{h \in H}} P(D_{train} \ is \ bad \ due \ to \ h)  \leq \sum_{\substack{h \in H}} P(D_{train} \ is \ bad  \ due \ to \ h)  

\end{array}$$

+++info 如何算得L(h, D<sub>all</sub>)? 

在所有可以收集到的资料上计算$l(h, x^n, y^n)$，求和取平均就能得到$L(h, D_{all})$

+++

+++info 如何算得L(h, D<sub>train</sub>)? 

从所有资料中取样出一些数据集。计算$l(h, x^n, y^n)$，求和取平均就能得到$L(h, D_{train})$

+++

### Hoeffding's inequality

> 霍夫丁不等式给出了随机变量的和与其期望值偏差的概率上限

$$ \begin {array}{c}

 P(D_{train} \ is \ bad  \ due \ to \ h) \leq 2 e^{-2N \varepsilon ^2} 

\end{array}$$

* L的范围是[0,1]
* N是$D_{train}$中训练资料的数目

所以可以得出获得的$D_{train}$是坏的概率为：

$$ \begin {array}{c}

P(D_{train} \ is \ bad) \leq \sum_{\substack{h \in H}} 2 e^{(-2N \varepsilon ^2)}    = |H|\cdot 2 e^ {(-2N \varepsilon ^2)} 

\end{array}$$

由此，让$P(D_{train} \ is \ bad)$变小，可以增大N或者缩小|H|。

!!也就是取得样本集中样本越多，h取值值范围越小，该集合是坏的概率越低!!

+++info 若h是连续的，那怎么确定|H|？

* 答案1：计算机中所有数据都是离散的
* 答案2：VC-dimension，一种描述模型复杂程度的指标

+++

为了达到理想的情况，我们可以增大N或缩小|H|，此时会面临另外两种问题

* 收集训练资料时，N是很难控制的
* 缩小h的范围，很难找到最合适的h值
  * 若|H|较大，则$L(h^{all}, D_{all})$会较小，也就是理想情况下损失较小，导致$L(h^{train}, D_{all})$与$L(h^{all}, D_{all})$的差距明显，理想情况下与现实情况有较明显的差距
  * 若|H|较小，则$L(h^{all}, D_{all})$会较大，也就是理想情况下损失较大，导致$L(h^{train}, D_{all})$与$L(h^{all}, D_{all})$的差距较小，理想情况下与现实情况差距不明显

如何解决这个问题？ 鱼与熊掌能不能兼得？

!!可以，深度学习!!